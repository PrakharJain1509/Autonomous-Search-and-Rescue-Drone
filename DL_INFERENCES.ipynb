{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-19T14:14:28.489420Z",
     "start_time": "2025-05-19T14:14:26.014570Z"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# â”€â”€â”€ PATHS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT = \"/Users/prakharjain/PycharmProjects/Drone_Git/Autonomous-Search-and-Rescue-Drone/Drone-UI\"\n",
    "MODEL_PATHS = {\n",
    "    \"image\": os.path.join(ROOT, \"models/image.pt\"),\n",
    "    \"thermal\": os.path.join(ROOT, \"models/thermal.pt\"),\n",
    "    \"audio\": os.path.join(ROOT, \"models/screaming_detector_gpu.pth\"),\n",
    "}\n",
    "INPUT_PATHS = {\n",
    "    \"image\": os.path.join(ROOT, \"test_data/image/human/Screenshot (379).png\"),\n",
    "    \"thermal\": os.path.join(ROOT, \"test_data/thermal/human/FLIR_07538_jpeg_jpg.rf.62bf28b984602aa4263620ad277675c8.jpg\"),\n",
    "    \"audio\": os.path.join(ROOT, \"test_data/audio/human/PES University Road 5.m4a\"),\n",
    "}\n",
    "\n",
    "# â”€â”€â”€ AUDIO MODEL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, 7, 2, 3, bias=False)\n",
    "        in_f = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_f, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Helper to rename keys in state_dict from \"resnet.\" to \"backbone.\"\n",
    "def convert_state_dict_keys(state_dict):\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"resnet.\"):\n",
    "            new_key = k.replace(\"resnet.\", \"backbone.\")\n",
    "        else:\n",
    "            new_key = k\n",
    "        new_state_dict[new_key] = v\n",
    "    return new_state_dict\n",
    "\n",
    "# â”€â”€â”€ IMAGE DISPLAY FUNCTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def show_img(img, title=\"\"):\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# â”€â”€â”€ INFERENCE FUNCTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_yolo_inference(model_path, image_path, label=\"YOLO Output\"):\n",
    "    model = YOLO(model_path)\n",
    "    results = model(image_path)\n",
    "    img = results[0].plot()\n",
    "    show_img(img, title=label)\n",
    "\n",
    "def run_audio_inference(model_path, audio_path):\n",
    "    model = AudioClassifier()\n",
    "    state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "    converted_dict = convert_state_dict_keys(state_dict)\n",
    "    model.load_state_dict(converted_dict)\n",
    "    model.eval()\n",
    "\n",
    "    wave, sr = torchaudio.load(audio_path)\n",
    "    target_sr, dur = 22050, 22050 * 3\n",
    "\n",
    "    if sr != target_sr:\n",
    "        wave = torchaudio.transforms.Resample(sr, target_sr)(wave)\n",
    "    wave = wave.mean(dim=0, keepdim=True) if wave.ndim > 1 else wave\n",
    "    if wave.shape[-1] < dur:\n",
    "        wave = torch.nn.functional.pad(wave, (0, dur - wave.shape[-1]))\n",
    "    else:\n",
    "        wave = wave[..., :dur]\n",
    "\n",
    "    mel = torchaudio.transforms.MelSpectrogram(sample_rate=target_sr, n_mels=64)(wave)\n",
    "    spec = torch.log(mel + 1e-9).unsqueeze(0)\n",
    "\n",
    "    output = model(spec)\n",
    "    pred = \"Human Detected\" if int(output.argmax()) == 1 else \"No Human\"\n",
    "    print(f\"ðŸ”Š Audio Inference Result: {pred}\")\n",
    "\n",
    "# â”€â”€â”€ RUN ALL INFERENCES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "run_yolo_inference(MODEL_PATHS[\"image\"], INPUT_PATHS[\"image\"], label=\"Image Model Output\")\n",
    "run_yolo_inference(MODEL_PATHS[\"thermal\"], INPUT_PATHS[\"thermal\"], label=\"Thermal Model Output\")"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T13:50:21.656246Z",
     "start_time": "2025-05-19T13:50:21.650220Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d073d42e748f9512",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T13:50:08.536069Z",
     "start_time": "2025-05-19T13:50:08.532939Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "559f1dcf2d73fe8d",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "a0edcfae06fa928c",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
